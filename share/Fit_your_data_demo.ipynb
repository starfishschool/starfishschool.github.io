{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From \"Chi by Eye\" to MCMC: An Introduction to Estimating Errors in Astronomy\n",
    "## Starfish School 2022\n",
    "### By: Joshua S. Speagle & Ting S. Li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only necessary if you're running Python 2.7 or lower\n",
    "from __future__ import print_function, division\n",
    "from six.moves import range\n",
    "\n",
    "# import matplotlib and define our alias\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# plot figures within the notebook rather than externally\n",
    "%matplotlib inline\n",
    "\n",
    "# numpy\n",
    "import numpy as np\n",
    "\n",
    "# scipy \n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will try to give an introduction to statistics and error estimation in astronomy. Through the notebook, we will go through a simple **linear regression** example that appears simple at first glance, but actually has a surprising amount of depth and is applicable in a bunch of different domains. \n",
    "\n",
    "Most importantly, it provides an accessible way to get a handle on several big concepts in **Bayesian Inference**, including:\n",
    "- defining objective functions,\n",
    "- probabilities and likelihoods, \n",
    "- estimating parameters,\n",
    "- the concept of priors, and\n",
    "- marginalizing over uncertainties.\n",
    "\n",
    "By the end of the talk, we'll have built up to the concepts behind **Markov Chain Monte Carlo (MCMC)**, which some of you may have heard of and which has become pretty ubiquitous in astronomy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate some mock data below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random number seed to ensure reproducibility\n",
    "seed = 123\n",
    "rstate = np.random.RandomState(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_mock_data(m=0.875, b=2.523, s=0.523, unc=[0.2, 0.6], N=20):\n",
    "    \"\"\"\n",
    "    Generate `N` mock data points from the line\n",
    "    with slope `m`, intercept `b`, and\n",
    "    intrinsic scatter `s` with measurement uncertainties\n",
    "    uniformly distributed between the values in `unc` using\n",
    "    a random number generator with the starting `seed`.\n",
    "    \n",
    "    \"\"\"\n",
    "        \n",
    "    # generate synthetic data\n",
    "    x = np.sort(3. * rstate.rand(N))  # x values\n",
    "    y_int = m * x + b  # underlying line\n",
    "    y = np.random.normal(y_int, s)  # with intrinsic scatter\n",
    "    yerr = np.random.uniform(unc[0], unc[1], N)  # measurement errors\n",
    "    yobs = np.random.normal(y, yerr)\n",
    "    \n",
    "    return x, yobs, yerr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate mock data\n",
    "x, y, ye = gen_mock_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot data\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.errorbar(x, y, yerr=ye, fmt=\"ko\", ecolor='gray', capsize=0)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('Session5_data.txt', np.c_[x, y, ye])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our starting goal here is going to be pretty simple: **try to determine a simple linear relationship**. \n",
    "\n",
    "In other words, we want a model like\n",
    "\n",
    "$$ y = mx + b $$\n",
    "\n",
    "where $m$ is the slope and $b$ is the $y$-intercept. We will slowly make this model more complicated throughout the talk as we try and model additional features of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Ocular Least Squares\" / \"Chi by Eye\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, let's ignore all attempts to make this quantitative. The human brain is actually quite good at fitting models to data, so we can do an **\"ocular least squares\"** process (based on the \"ordinary least squares\" approach we will define shortly) or **\"chi by eye\"** process (based on the \"chi-squared\" statistic we will define later) and just try and benchmark what looks like a reasonable fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best estimates of slope and intercept\n",
    "m_1, b_1 = 1.25, 2.\n",
    "\n",
    "# plot results\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.errorbar(x, y, yerr=ye, fmt=\"ko\", ecolor='gray', \n",
    "             capsize=0, label='Data')  # plot data\n",
    "plt.plot(x, m_1 * x + b_1, lw=4, alpha=0.7, color='red',\n",
    "         label='by eye')  # plot chi-by-eye\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our guess probably looks \"reasonable\", but you might not feel so good about it. Why? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you say *why* it's the best fit? How about uncertainties on the slope and intercept?\n",
    "\n",
    "Part of the difficulty here is that there is no quantitative metric for what makes a fit \"good\". We will now try to rectify this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective/Loss/Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to define whether a fit is any good is to see how well it actually *predicts* the data it is supposed to be fitting. One way to quantify this involves looking at the residuals\n",
    "\n",
    "$$ |\\Delta y_i| = |y_{i, {\\rm pred}} - y_{i}| $$\n",
    "\n",
    "where $i$ indicates the $i$th datapoint, $y_{i, {\\rm pred}} = m x_i + b$ is the prediction from the model and $y_{i}$ is the observed (noisy) value. We are interested in the absolute value because what *probably* matters is the total error, not whether it's positive or negative. *(Note that we are currently ignoring the errors.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would ideally like to have a model that has the smallest residuals for all the points. Let's therefore define a **loss function** (also often called an **objective function** or a **cost function**):\n",
    "\n",
    "$$ L(m, b | \\{ (x_1, y_1), \\dots, (x_N, y_N) \\}) = \\sum_{i=1}^{N} |\\Delta y_i|^p $$\n",
    "\n",
    "where the $|$ line now indicates \"given\" or \"conditioned on\". In other words, this equation reads:\n",
    "\n",
    "> \"What is the loss of a particular slope $m$ and intercept $b$ given the data $\\{ (x_1, y_1), \\dots, (x_N, y_N) \\}$?\"\n",
    "\n",
    "$p$ is a power that all residuals are raised to that control how sensitive the loss function is to large and small residuals. Common values for $p$ are often $1 \\leq p \\leq 2$, with $p=2$ (squaring the residuals) being the most common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "def loss(theta, p=2., x=x, y=y):\n",
    "    \"\"\"A simple loss function as a function of parameters `theta`.\"\"\"\n",
    "    \n",
    "    m, b = theta  # reassign parameters\n",
    "    ypred = m * x + b\n",
    "    resid = np.abs(ypred - y)\n",
    "    \n",
    "    return np.sum(resid**p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change m at fixed b\n",
    "mgrid = np.linspace(0, 2, 100)\n",
    "loss_m = np.array([loss([m, b_1], p=2.) for m in mgrid])\n",
    "    \n",
    "# change b at fixed m\n",
    "bgrid = np.linspace(0, 6, 100)\n",
    "loss_b = np.array([loss([m_1, b], p=2.) for b in bgrid])\n",
    "\n",
    "\n",
    "# plot results\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(mgrid, loss_m, lw=3, color='firebrick')  # varying m given b\n",
    "plt.xlabel('m (fixed b)')\n",
    "plt.ylabel('Loss')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(bgrid, loss_b, lw=3, color='navy')  # varying b given m\n",
    "plt.xlabel('b (fixed m)')\n",
    "plt.ylabel('Loss')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the best fit, we want to **minimize our loss** (alternately, **optimize our objective**).\n",
    "\n",
    "We will use the `minimize` function from `scipy.optimize` (see [documentation](https://docs.scipy.org/doc/scipy-0.19.0/reference/generated/scipy.optimize.minimize.html)) to get the best-fit parameters `theta` based on our loss function `loss` and quantify the difference relative to our initial results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "# minimize the loss function\n",
    "results = minimize(loss, [m_1, b_1])\n",
    "\n",
    "# print the results\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get best-fit result\n",
    "m_2, b_2 = results['x']\n",
    "\n",
    "# plot results\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.errorbar(x, y, yerr=ye, fmt=\"ko\", ecolor='gray', \n",
    "             capsize=0, label='Data')  # plot data\n",
    "plt.plot(x, m_1 * x + b_1, lw=4, alpha=0.4, color='red',\n",
    "         label='by eye')  # plot chi-by-eye\n",
    "plt.plot(x, m_2 * x + b_2, lw=4, alpha=0.7, color='dodgerblue',\n",
    "         label='loss')  # plot minimum loss\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantifying the improvement\n",
    "print('Change in Loss:', loss([m_1, b_1]) - loss([m_2, b_2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incorporating Errors with Chi-Square"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we haven't tried to incorporate **errors** into our linear regression model. We probably want our fit to take these into account, especially if the errors can change from point to point! Points with large errors probably should be \"down-weighted\" in the fit since they are more uncertain. We can accomplish this be defining the **error-normalized resuidual** $\\chi$:\n",
    "\n",
    "$$ \\chi_i = \\left|\\frac{\\Delta y_i}{\\sigma_i}\\right| $$\n",
    "\n",
    "where $\\sigma_i$ is the measurement error associated with $y_i$. This should make some intuitive sense: we are just normalizing the residual by the error, so now we are measuring how discrepant the prediction is in units of the error bar for each point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define the **chi-square statistic**:\n",
    "\n",
    "$$ \\chi^2(m, b | \\{ (x_1, y_1, \\sigma_1), \\dots, (x_N, y_N, \\sigma_N) \\}) = \\sum_{i=1}^{N} \\chi^2_i $$\n",
    "\n",
    "You may have heard this term being used before to describe the quality of a fit to data, since the $\\chi^2$ statistic is used quite often in the astronomical literature!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chi2 function\n",
    "def chi2(theta, x=x, y=y, ye=ye):\n",
    "    \"\"\"Chi-square as a function of parameters `theta`.\"\"\"\n",
    "    \n",
    "    m, b = theta  # reassign parameters\n",
    "    ypred = m * x + b\n",
    "    resid = ypred - y\n",
    "    \n",
    "    return np.sum(resid**2 / ye**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minimize chi2\n",
    "results = minimize(chi2, [m_2, b_2])\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get best-fit result\n",
    "m_3, b_3 = results['x']\n",
    "\n",
    "# plot results\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.errorbar(x, y, yerr=ye, fmt=\"ko\", ecolor='gray', \n",
    "             capsize=0, label='Data')  # plot data\n",
    "plt.plot(x, m_1 * x + b_1, lw=4, alpha=0.4, color='red',\n",
    "         label='by eye')  # plot chi-by-eye\n",
    "plt.plot(x, m_2 * x + b_2, lw=4, alpha=0.4, color='dodgerblue',\n",
    "         label='loss')  # plot minimum loss\n",
    "plt.plot(x, m_3 * x + b_3, lw=4, alpha=0.7, color='darkviolet',\n",
    "         label='chi2')  # plot minimum chi2\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantifying the improvement\n",
    "print('Change in chi2:', \n",
    "      chi2([m_1, b_1]) - chi2([m_3, b_3]), \n",
    "      chi2([m_2, b_2]) - chi2([m_3, b_3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Loss Functions to Probabilistic Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to consider an additional source of uncertainty: some amount of **intrinsic scatter** in the fitted relationship. In other words, in addition to the scatter $\\sigma_i$ caused by the observational uncertainties, we also want to add on an additional scatter $s_i = s$ that is the same for all points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How might we do so? Well, maybe we could try something like\n",
    "\n",
    "$$\\chi = \\left|\\frac{\\Delta y_i}{\\sigma_i + s}\\right|$$\n",
    "\n",
    "Notice, however, that $\\chi^2$ will always get smaller as $s$ increases. Whoops! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we add on a penalty to disfavor large values? Maybe something like\n",
    "\n",
    "$$ \\chi^2(m, b, s | \\{ (x_1, y_1, \\sigma_1), \\dots, (x_N, y_N, \\sigma_N) \\}) = s^2 + \\sum_{i=1}^{N} \\chi^2_i $$\n",
    "\n",
    "could work. We could even argue for reasons why it might be reasonable!\n",
    "\n",
    "This basic result, however, reveals a *fundamental problem* with how we're approached fitting a line so far: we haven't actually defined an underlying **probabilistic model** for the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Likelihoods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All models start with trying to understand what I like to call the **data generating process**. In other words, if we wanted to simulate data based on an input set of parameters, how would we do so?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability Density Functions (PDFs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do so, let's start with the observed $y$ values. We have **observational uncertainties**, but what do those *mean*? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, in general it means we assume the observed data follow a **Normal distribution** (also called a **Gaussian distribution**) such that the probability that $y_i$ is any particular value follows a **probability density function (PDF)** of the form\n",
    "\n",
    "$$ P(y_i|y_{i,{\\rm true}}, \\sigma_i) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left[-\\frac{1}{2}\\frac{(y_{i, {\\rm true}} - y_i)^2}{\\sigma_i^2}\\right] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our model for the data is $y_{i, {\\rm true}} = m x_i + b$, this then gives\n",
    "\n",
    "$$ P(y_i|m, b, x_i, \\sigma_i) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left[-\\frac{1}{2}\\frac{(m x_i + b - y_i)^2}{\\sigma_i^2}\\right] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although it is beyond the scope of this notebook to prove, it can be shown that assuming that the intrinsic scatter $s$ is also Gaussian gives the modified result:\n",
    "\n",
    "$$ P(y_i|m, b, s, x_i, \\sigma_i) = \\frac{1}{\\sqrt{2\\pi(\\sigma_i^2 + s^2)}} \\exp\\left[-\\frac{1}{2}\\frac{(m x_i + b - y_i)^2}{\\sigma_i^2 + s^2}\\right] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability of *independent* events $A$, $B$, and $C$ all occuring is just the product of their individual probabilities:\n",
    "\n",
    "$$ P(A, B, C) = P(A) P(B) P(C) $$\n",
    "\n",
    "The same holds true for data points: if each data point is an independent observation, the total probability is just the individual probabilities for each data point multiplied together. This gives\n",
    "\n",
    "$$ P(y_1, \\dots, y_N|m, b, s, (x_1, \\sigma_1), \\dots, (x_N, \\sigma_N)) = \\prod_{i=1}^{N} P(y_i|m, b, s, x_i, \\sigma_i) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the probabilities can get really small really fast, for numerical stability this is almost always re-written in logarithmic form:\n",
    "\n",
    "$$ \\ln P(y_1, \\dots, y_N|m, b, s, (x_1, \\sigma_1), \\dots, (x_N, \\sigma_N)) = \\sum_{i=1}^{N} \\ln P(y_i|m, b, s, x_i, \\sigma_i) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that, if we explicitly substitute in the Gaussian probability density function, we get this intriguing result:\n",
    "\n",
    "$$ \\ln P(y_1, \\dots, y_N|m, b, s, (x_1, \\sigma_1), \\dots, (x_N, \\sigma_N)) = -\\frac{1}{2} \\sum_{i=1}^{N} \\frac{(mx_i + b - y_i)^2}{\\sigma_i^2 + s^2} + \\ln(2\\pi(\\sigma_i^2 + s^2)) $$\n",
    "\n",
    "The first term here looks a lot like our original $\\chi^2$ expression, except now with the modified errors. And the second term now looks a lot like a penalty term that disfavors larger $s$ values! \n",
    "\n",
    "*By explicitly writing out a model, we naturally accomplish our original objective!* A lot of neat results often happen this way -- when you're struggling with trying to fit a dataset, going back to the fundamentals often can reveal a lot of neat results like this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum-Likelihood Estimate (MLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"best-fit\" parameters now can be defined as those that **maximize the likelihood** (or, equivalently, **minimize the negative of the likelihood**). Since logarithms don't change the maximum or minimum, we often use the log-likelihoods in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negloglike(theta, x=x, y=y, ye=ye):\n",
    "    \"\"\"(Negative) log-likelihood as a function of parameters `theta`.\"\"\"\n",
    "    \n",
    "    m, b, s = theta  # reassign parameters\n",
    "    ypred = m * x + b\n",
    "    resid = ypred - y\n",
    "    chi2 = resid**2 / (ye**2 + s**2)  # chi2 term\n",
    "    const = np.log(2 * np.pi * (ye**2 + s**2))  # normalization/penalty term\n",
    "    logl = -0.5 * np.sum(chi2 + const)\n",
    "    \n",
    "    return -logl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minimize negative log-likelihood\n",
    "results = minimize(negloglike, [m_3, b_3, 0.5])\n",
    "results_mle = results\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get best-fit result\n",
    "m_4, b_4, s_4 = results['x']\n",
    "\n",
    "# plot results\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.errorbar(x, y, yerr=ye, fmt=\"ko\", ecolor='gray', \n",
    "             capsize=0, label='Data')  # plot data\n",
    "plt.plot(x, m_1 * x + b_1, lw=4, alpha=0.4, color='red',\n",
    "         label='by eye')  # plot chi-by-eye\n",
    "plt.plot(x, m_2 * x + b_2, lw=4, alpha=0.4, color='dodgerblue',\n",
    "         label='loss')  # plot minimum loss\n",
    "plt.plot(x, m_3 * x + b_3, lw=4, alpha=0.4, color='darkviolet',\n",
    "         label='chi2')  # plot minimum chi2\n",
    "plt.plot(x, m_4 * x + b_4, lw=4, alpha=0.7, color='darkorange', \n",
    "         label='MLE')  # plot MLE\n",
    "[plt.fill_between(x, m_4 * x + b_4 + s_4 * n, m_4 * x + b_4 - s_4 * n, \n",
    "                  alpha=0.02, color='darkorange')\n",
    " for n in np.linspace(0, 2, 20)]  # plot MLE uncertainty estimates\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantifying the improvement\n",
    "print('Change in loglike:', \n",
    "      negloglike([m_3, b_3, 0]) - negloglike([m_4, b_4, s_4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a working model that can describe the underlying probabilistic processes that generate the data. However, we're still missing one small thing. \n",
    "\n",
    "In our earlier notation,\n",
    "\n",
    "$$ P(A | B) $$\n",
    "\n",
    "describes the probability of $A$ *given* (i.e. conditioned on) $B$. So our likelihood\n",
    "\n",
    "$$ P(y_1, \\dots, y_N | m, b, s, (x_1, \\sigma_1), \\dots, (x_N, \\sigma_N)) $$\n",
    "\n",
    "describes something similar: the probability of seeing the observed *data* $\\{ y_1, \\dots, y_N\\}$ given the underlying model parameters $m$, $b$, and $s$ along with the $x_i$ and $\\sigma_i$ values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we *want*, however, is this expression:\n",
    "\n",
    "$$ P(m, b, s | (x_1, y_1, \\sigma_1), \\dots, (x_N, y_N, \\sigma_N)) $$\n",
    "\n",
    "This now describes the **probability of our *model parameters* given the data**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes' Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting at this distribution requires exploiting the fact that probabilities can factor. This allows us to rewrite:\n",
    "\n",
    "$$ P(A, B) = P(A|B)P(B) = P(B|A)P(A) = P(B, A) $$\n",
    "\n",
    "Rearranging then gives:\n",
    "\n",
    "$$ P(B|A) = \\frac{P(A|B) P(B)}{P(A)} $$\n",
    "\n",
    "If we replace $A = {\\rm data}$ and $B = {\\rm parameters}$, we are left with what's known as **Bayes' Theorem**:\n",
    "\n",
    "$$ P({\\rm parameters} \\,|\\, {\\rm data}) = \\frac{P({\\rm data}\\,|\\,{\\rm parameters}) \\, P({\\rm parameters})}{P({\\rm data})} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pulling apart this expression:\n",
    "- The term $P({\\rm data}\\,|\\,{\\rm parameters})$, is the **likelihood** that we have been working with already and describes the probability of seeing the data given the model. \n",
    "- The term $P({\\rm parameters})$ is known as the **prior**: it characterizes our prior knowledge about the parameters is question without seeing the data. \n",
    "- The term $P({\\rm data})$ is known as the **evidence** (or marginal likelihood). Since this is just a constant, most often we can just ignore it. (It is useful though! Feel free to ask me about it.)\n",
    "- Finally, the left-hand side $P({\\rm parameters} \\,|\\, {\\rm data})$ is known as the **posterior** since it combines the prior and the likelihood together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Maximum-a-Posteriori* (MAP) Estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can now find our parameter estimates with the highest log-posterior values, known as the **maximum-a-posteriori** (MAP) estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a Gaussian prior over each parameter\n",
    "prior_means = np.array([1., 3., 0.5])  # m, x, b\n",
    "prior_stds = np.array([0.25, 0.5, 0.15])  # m, x, b\n",
    "\n",
    "def neglogprior(theta, mean=prior_means, std=prior_stds):\n",
    "    \"\"\"(Negative) log-prior as a function of parameters `theta`.\"\"\"\n",
    "    \n",
    "    chi2 = (theta - mean)**2 / std**2\n",
    "    const = np.log(2. * np.pi * std**2)\n",
    "    logp = -0.5 * np.sum(chi2 + const)\n",
    "    \n",
    "    return -logp\n",
    "\n",
    "def neglogpost(theta, x=x, y=y, ye=ye, mean=prior_means, std=prior_stds):\n",
    "    \"\"\"(Negative) log-posterior as a function of parameters `theta`.\"\"\"\n",
    "    \n",
    "    m, b, s = theta  # reassign parameters\n",
    "    logp = -neglogprior(theta, mean=mean, std=std)  # prior\n",
    "    logl = -negloglike(theta, x=x, y=y, ye=ye)  # likelihood\n",
    "    \n",
    "    return -(logl + logp)  # posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minimize negative log-posterior\n",
    "results = minimize(neglogpost, [m_4, b_4, s_4])\n",
    "results_map = results\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get best-fit result\n",
    "m_5, b_5, s_5 = results['x']\n",
    "\n",
    "# plot it against previous results\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.errorbar(x, y, yerr=ye, fmt=\"ko\", ecolor='gray', \n",
    "             capsize=0, label='Data')  # plot data\n",
    "plt.plot(x, m_1 * x + b_1, lw=4, alpha=0.4, color='red',\n",
    "         label='by eye')  # plot chi-by-eye\n",
    "plt.plot(x, m_2 * x + b_2, lw=4, alpha=0.4, color='dodgerblue',\n",
    "         label='loss')  # plot minimum loss\n",
    "plt.plot(x, m_3 * x + b_3, lw=4, alpha=0.4, color='darkviolet',\n",
    "         label='chi2')  # plot minimum chi2\n",
    "plt.plot(x, m_4 * x + b_4, lw=4, alpha=0.4, color='darkorange', \n",
    "         label='MLE')  # plot MLE (no priors)\n",
    "plt.plot(x, m_5 * x + b_5, lw=4, alpha=0.7, color='gray', \n",
    "         label='MAP')  # plot MAP (including priors)\n",
    "[plt.fill_between(x, m_5 * x + b_5 + s_5 * n, m_5 * x + b_5 - s_5 * n, \n",
    "                  alpha=0.02, color='gray')\n",
    " for n in np.linspace(0, 2, 20)]  # plot MAP uncertainty estimates\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Uncertainties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our probability distributions can be used to estimate more than just best-fit parameters. They also give us estimates of the uncertainties by telling us just how much more \"probable\" one set of parameters is compared to another. Exploring this uncertainty is hard though, since a function like\n",
    "\n",
    "$$ \\ln P(m, b, s | y_1, \\dots, y_N, (x_1, \\sigma_1), \\dots, (x_N, \\sigma_N)) $$\n",
    "\n",
    "doesn't just come with labels attached for how the (log-)probability changes as a function of the parameters!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inverse Hessian (Gaussian) Approxmation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While exploring this fully is beyond the scope of this introduction, one neat result of using a properly defined probabilistic model is that `scipy.minimize` provides estimates of the parameter errors! This can be accessed via the `hess_inv` item in the output results dictionary. We can interpret this as essentially a \"Gaussian\" approximation: these are what the errors would be if we assume the unknown distribution was roughly Gaussian centered around the best-fit parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(1e6)  # number of samples to draw\n",
    "\n",
    "# generate prior samples\n",
    "m_prior, b_prior, s_prior = np.random.multivariate_normal(prior_means, \n",
    "                                                          np.diag(prior_stds**2), \n",
    "                                                          size=n).T\n",
    "\n",
    "# generate likelihood samples\n",
    "m_like, b_like, s_like = np.random.multivariate_normal(results_mle['x'], \n",
    "                                                       results_mle['hess_inv'], \n",
    "                                                       size=n).T\n",
    "\n",
    "# generate posterior samples\n",
    "m_post, b_post, s_post = np.random.multivariate_normal(results_map['x'], \n",
    "                                                       results_map['hess_inv'], \n",
    "                                                       size=n).T\n",
    "\n",
    "# define bins\n",
    "bins = [np.linspace(0, 2, 100),  # bins (m)\n",
    "        np.linspace(1, 5, 100),  # bins (b)\n",
    "        np.linspace(0, 1, 100)]  # bins (s)\n",
    "\n",
    "# generate 1-D histograms of priors vs posteriors\n",
    "plt.figure(figsize=(15, 10))\n",
    "# slope (m)\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.hist(m_prior, bins[0], density=True, label='prior',\n",
    "         color='dodgerblue', alpha=0.6)\n",
    "plt.hist(m_like, bins[0], density=True, label='like',\n",
    "         color='orange', alpha=0.6)\n",
    "plt.hist(m_post, bins[0], density=True, label='post',\n",
    "         color='firebrick', alpha=0.6)\n",
    "plt.xlim(bins[0][0], bins[0][-1])\n",
    "plt.xlabel('m')\n",
    "plt.ylabel('Probability')\n",
    "plt.yticks([])\n",
    "plt.legend()\n",
    "# y-intercept (b)\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.hist(b_prior, bins[1], density=True, label='prior',\n",
    "         color='dodgerblue', alpha=0.6)\n",
    "plt.hist(b_like, bins[1], density=True, label='like',\n",
    "         color='orange', alpha=0.6)\n",
    "plt.hist(b_post, bins[1], density=True, label='post',\n",
    "         color='firebrick', alpha=0.6)\n",
    "plt.xlim(bins[1][0], bins[1][-1])\n",
    "plt.xlabel('b')\n",
    "plt.ylabel('Probability')\n",
    "plt.yticks([])\n",
    "# scatter (s)\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.hist(s_prior, bins[2], density=True, label='prior',\n",
    "         color='dodgerblue', alpha=0.6)\n",
    "plt.hist(s_like, bins[2], density=True, label='like',\n",
    "         color='orange', alpha=0.6)\n",
    "plt.hist(s_post, bins[2], density=True, label='post',\n",
    "         color='firebrick', alpha=0.6)\n",
    "plt.xlim(bins[2][0], bins[2][-1])\n",
    "plt.xlabel('s')\n",
    "plt.ylabel('Probability')\n",
    "plt.yticks([])\n",
    "plt.tight_layout()\n",
    "\n",
    "# generate 2-D histograms of posteriors\n",
    "\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.hist2d(m_post, b_post, [bins[0], bins[1]],\n",
    "           cmap='Reds', alpha=0.8)\n",
    "plt.hist2d(m_like, b_post, [bins[0], bins[1]],\n",
    "           cmap='Oranges', alpha=0.6)\n",
    "plt.hist2d(m_prior, b_prior, [bins[0], bins[1]],\n",
    "           cmap='Blues', alpha=0.3)\n",
    "plt.xlabel('m')\n",
    "plt.ylabel('b')\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.hist2d(b_post, s_post, [bins[1], bins[2]],\n",
    "           cmap='Reds', alpha=0.8)\n",
    "plt.hist2d(b_like, s_post, [bins[1], bins[2]],\n",
    "           cmap='Oranges', alpha=0.6)\n",
    "plt.hist2d(b_prior, s_prior, [bins[1], bins[2]],\n",
    "           cmap='Blues', alpha=0.3)\n",
    "plt.xlabel('b')\n",
    "plt.ylabel('s')\n",
    "plt.subplot(2, 3, 6)\n",
    "plt.hist2d(s_post, m_post, [bins[2], bins[0]],\n",
    "           cmap='Reds', alpha=0.8)\n",
    "plt.hist2d(s_like, m_post, [bins[2], bins[0]],\n",
    "           cmap='Oranges', alpha=0.6)\n",
    "plt.hist2d(s_prior, m_prior, [bins[2], bins[0]],\n",
    "           cmap='Blues', alpha=0.3)\n",
    "plt.xlabel('s')\n",
    "plt.ylabel('m')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to tyy and visualize all of this is through the use of a **corner plot** (also known as a triangle plot). The `corner` package (see [documentation](https://corner.readthedocs.io/en/latest/index.html)) provides an easy to use interface to visualize the density of samples in 1-D and 2-D, like above, in one easy place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import corner\n",
    "\n",
    "# plot posterior\n",
    "corner.corner(np.c_[m_post, b_post, s_post],  # collect samples into N x 3 array\n",
    "              bins=50,  # bins for histogram\n",
    "              labels=['m', 'b', 's'],\n",
    "              show_titles=True, quantiles=[0.16, 0.84],  # show median and uncertainties\n",
    "              truths=results_map['x'],  # plot MAP\n",
    "              color='firebrick', truth_color='black',  # add some colors\n",
    "              **{'plot_datapoints': False, 'fill_contours': True});  # change some default options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Chain Monte Carlo (MCMC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we get to **Markov Chain Monte Carlo (MCMC)**. For this, I'm going to focus on generating a few plots highlighted in a paper I wrote a while back ([Speagle 2019](https://arxiv.org/pdf/1909.12313.pdf)) to showcase how it works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we will use is an implementation provided as part of the `emcee` package (see [documentation](https://emcee.readthedocs.io/en/stable/))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logpost(theta, x=x, y=y, ye=ye, mean=prior_means, std=prior_stds):\n",
    "    \"\"\"(Negative) log-posterior as a function of parameters `theta`.\"\"\"\n",
    "    \n",
    "    m, b, s = theta  # reassign parameters\n",
    "    logp = -neglogprior(theta, mean=mean, std=std)  # prior\n",
    "    logl = -negloglike(theta, x=x, y=y, ye=ye)  # likelihood\n",
    "    \n",
    "    return (logl + logp)  # posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emcee\n",
    "\n",
    "ndim = 3  # number of parameters\n",
    "nwalkers = 100  # number of \"walkers\" or \"chains\" to run\n",
    "\n",
    "# initialize starting positions from our initial approximation\n",
    "p0 = np.c_[m_post, b_post, s_post][:nwalkers]\n",
    "\n",
    "# initialize our sampler\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, logpost)\n",
    "\n",
    "# initial \"burn-in\" run\n",
    "state = sampler.run_mcmc(p0, 1000, progress=True)\n",
    "sampler.reset()\n",
    "\n",
    "# final \"production\" run\n",
    "state = sampler.run_mcmc(state, 10000, progress=True)\n",
    "\n",
    "# get final chains\n",
    "samples = sampler.get_chain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a few chains\n",
    "plt.figure(figsize=(14, 9))\n",
    "# slope (m)\n",
    "plt.subplot(3, 1, 1)\n",
    "[plt.plot(samples[:, i, 0], alpha=0.5) for i in range(4)]\n",
    "plt.xlim([0, 5000])\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('m')\n",
    "# intercept (b)\n",
    "plt.subplot(3, 1, 2)\n",
    "[plt.plot(samples[:, i, 0], alpha=0.5) for i in range(4)]\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('b')\n",
    "plt.xlim([0, 5000])\n",
    "# scatter (s)\n",
    "plt.subplot(3, 1, 3)\n",
    "[plt.plot(samples[:, i, 0], alpha=0.5) for i in range(4)]\n",
    "plt.xlim([0, 5000])\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('s')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot posterior\n",
    "corner.corner(samples.reshape(-1, ndim),  # collect samples into N x 3 array\n",
    "              bins=50,  # bins for histogram\n",
    "              show_titles=True, quantiles=[0.16, 0.84],  # show median and uncertainties\n",
    "              labels=['m', 'b', 's'],\n",
    "              truths=results_map['x'],  # plot MAP\n",
    "              color='darkviolet', truth_color='black',  # add some colors\n",
    "              **{'plot_datapoints': False, 'fill_contours': True});  # change some default options"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
